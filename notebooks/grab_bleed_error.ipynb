{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Notebook to transform each flight into one line and agregating all of them to create one parquet file for each aircraft</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries needed\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import os\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduces the size of the dataframe by converting columns to smaller data types\n",
    "def convert_columns(df_filtered):\n",
    "    for column in df_filtered.columns:\n",
    "        if df_filtered[column].dtype == 'float64':\n",
    "            df_filtered[column] = df_filtered[column].astype('float32')\n",
    "\n",
    "        if df_filtered[column].dtype == 'int64':\n",
    "            df_filtered[column] = df_filtered[column].astype('int32')\n",
    "\n",
    "    return df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_files_in_directory(dirpath, output_directory):\n",
    "    # For storing error-related data\n",
    "    error_data_list = []\n",
    "    # For storing data related to short flights\n",
    "    short_flights_data_list = []\n",
    "\n",
    "    # Loop through all files in the directory and process them\n",
    "    for filename in os.listdir(dirpath):\n",
    "        if filename.endswith('.parquet'):\n",
    "            file_path = os.path.join(dirpath, filename)\n",
    "            try:\n",
    "                # Load the file\n",
    "                df = pd.read_parquet(file_path)\n",
    "\n",
    "                df = convert_columns(df)\n",
    "\n",
    "                # If the DataFrame has a column named \"erro\"\n",
    "                if \"message0418DAA-1\" in df.columns and \"message0422DAA-1\" in df.columns:\n",
    "\n",
    "                    error_count_erro1 = df[df['message0418DAA-1'].notna() & (df['message0418DAA-1'] != 0)].shape[0]\n",
    "                    error_count_erro2 = df[df['message0422DAA-1'].notna() & (df['message0422DAA-1'] != 0)].shape[0]\n",
    "                    unique_values_error1 = df['message0418DAA-1'].unique().tolist()\n",
    "                    unique_values_error2 = df['message0422DAA-1'].unique().tolist()\n",
    "\n",
    "                    if error_count_erro1 > 0 or error_count_erro2 > 0:\n",
    "                        error_data_list.append({\n",
    "                            \"nome\": filename,\n",
    "                            \"quantidade_message0418DAA-1\": copy.deepcopy(error_count_erro1),\n",
    "                            \"quantidade_message0422DAA-1\": copy.deepcopy(error_count_erro2),\n",
    "                            \"valores_error1\": copy.deepcopy(unique_values_error1),\n",
    "                            \"valores_error2\": copy.deepcopy(unique_values_error2)\n",
    "                        })\n",
    "\n",
    "                # Check if the DataFrame has less than 3.6e+6 rows\n",
    "                if df.shape[0] < 72000:\n",
    "                    short_flights_data_list.append({\n",
    "                        \"nome\": filename,\n",
    "                        \"tempo\": copy.deepcopy(df.shape[0])\n",
    "                    })\n",
    "\n",
    "                del df\n",
    "                gc.collect()\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading the file {file_path}: {e}\")\n",
    "\n",
    "    # Save the consolidated error data if it exists\n",
    "    if error_data_list:\n",
    "        error_df = pd.DataFrame(error_data_list)\n",
    "        error_path = os.path.join(output_directory, f\"Voos_com_erro\")\n",
    "        error_file = os.path.join(error_path, f\"voos_erro_{os.path.basename(dirpath)}.parquet\")\n",
    "        error_df.to_parquet(error_file)\n",
    "\n",
    "    # Save the consolidated short flights data if it exists\n",
    "    if short_flights_data_list:\n",
    "        short_flights_df = pd.DataFrame(short_flights_data_list)\n",
    "        short_path = os.path.join(output_directory, f\"Voos_curtos\")\n",
    "        short_flights_file = os.path.join(short_path, f\"voos_curtos_{os.path.basename(dirpath)}.parquet\")\n",
    "        short_flights_df.to_parquet(short_flights_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_cutted_flights(dirpath, output_directory):\n",
    "    required_navegacao_values = [2, 3, 5, 6, 7, 1]\n",
    "    required_fase_values = [0, 1, 2]\n",
    "    # For storing data related to short flights\n",
    "    missing_data_list = []\n",
    "\n",
    "    # Loop through all files in the directory and process them\n",
    "    for filename in os.listdir(dirpath):\n",
    "        if filename.endswith('.parquet'):\n",
    "            file_path = os.path.join(dirpath, filename)\n",
    "            try:\n",
    "                # Load the file\n",
    "                df = pd.read_parquet(file_path, columns=[\"phaseOfFlight-1\", \"phaseOfFlightNavigation-1\"])\n",
    "\n",
    "                unique_navegacao = df['phaseOfFlightNavigation-1'].unique().tolist()\n",
    "                unique_fase = df['phaseOfFlight-1'].unique().tolist()\n",
    "\n",
    "                # Check if all required values are present\n",
    "                if not all(val in unique_navegacao for val in required_navegacao_values) or \\\n",
    "                   not all(val in unique_fase for val in required_fase_values):\n",
    "                    \n",
    "                    missing_data_list.append({\n",
    "                        \"nome\": filename,\n",
    "                        \"phaseOfFlightNavigation-1\": unique_navegacao,\n",
    "                        \"phaseOfFlight-1\": unique_fase\n",
    "                    })\n",
    "\n",
    "            except Exception as e:\n",
    "               print(f\"Error reading the file {file_path}: {e}\")\n",
    "\n",
    "\n",
    "            if missing_data_list:\n",
    "               missing_df = pd.DataFrame(missing_data_list)\n",
    "               cutted_path = os.path.join(output_directory, f\"Voos_cortados\")\n",
    "               cutted_flights_file = os.path.join(cutted_path, f\"voos_cortados_{os.path.basename(dirpath)}.parquet\")\n",
    "               missing_df.to_parquet(cutted_flights_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to the data files\n",
    "directory_path = \"/Users/henriquematias/Documents/GitHub/Inteli-Modulo-7/Projeto_Grupo1_Inteli_Azul/notebooks/Datasets\"\n",
    "output_directory = \"/Users/henriquematias/Documents/GitHub/Inteli-Modulo-7/Projeto_Grupo1_Inteli_Azul/notebooks/Output\"\n",
    "\n",
    "# Making sure that the output directory exists\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Passing through all the files in all directories specified in the directory_path\n",
    "for dirpath, dirnames, filenames in os.walk(directory_path):\n",
    "    detect_cutted_flights(dirpath, output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>CÃ“DIGOS PARA USO POSTERIOR</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values = {}\n",
    "for column in df.columns:\n",
    "      unique_values[column] = df[column].unique()\n",
    "unique_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descarting columns that are not needed\n",
    "df = df.loc[:, columns_needed]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
